{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-To-End Automatic Speech Recognition with Nemo\n",
    "Basic tutorial of Automatic Speech Recognition (ASR) concepts, introduced with code snippets using the [NeMo framework](https://github.com/NVIDIA/NeMo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is ASR?\n",
    "\n",
    "- ASR, or **Automatic Speech Recognition**: Automatically transcribe spoken language (i.e., speech-to-text). \n",
    "- Our goal is usually to have a model that minimizes the **Word Error Rate (WER)** metric when transcribing speech input. \n",
    "    - Given some audio file (e.g. a WAV file) containing speech, how do we transform this into the corresponding text with as few errors as possible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a Look at Our Data (AN4)\n",
    "\n",
    "- The AN4 dataset, also known as the Alphanumeric dataset, was collected and published by Carnegie Mellon University. \n",
    "- It consists of recordings of people spelling out addresses, names, telephone numbers, etc., one letter or number at a time, as well as their corresponding transcripts. \n",
    "- We choose to use AN4 for this tutorial because it is relatively small, with 948 training and 130 test utterances, and so it trains quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download data and convert the .sph format to .wav format\n",
    "def download_an4_data(data_dir):\n",
    "    import glob\n",
    "    import os\n",
    "    import subprocess\n",
    "    import tarfile\n",
    "    import wget\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    # Download the dataset. This will take a few moments...\n",
    "    print(\"******\")\n",
    "    if not os.path.exists(data_dir + '/an4_sphere.tar.gz'):\n",
    "        an4_url = 'http://www.speech.cs.cmu.edu/databases/an4/an4_sphere.tar.gz'\n",
    "        an4_path = wget.download(an4_url, data_dir)\n",
    "        print(f\"Dataset downloaded at: {an4_path}\")\n",
    "    else:\n",
    "        print(\"Tarfile already exists.\")\n",
    "        an4_path = data_dir + '/an4_sphere.tar.gz'\n",
    "\n",
    "    # Untar and convert .sph to .wav (using sox)\n",
    "    tar = tarfile.open(an4_path)\n",
    "    tar.extractall(path=data_dir)\n",
    "\n",
    "    print(\"Converting .sph to .wav...\")\n",
    "    sph_list = glob.glob(data_dir + '/an4/**/*.sph', recursive=True)\n",
    "    for sph_path in sph_list:\n",
    "        wav_path = sph_path[:-4] + '.wav'\n",
    "        cmd = [\"sox\", sph_path, wav_path]\n",
    "        subprocess.run(cmd)\n",
    "    print(\"Finished conversion.\\n******\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where the an4/ directory will be placed.\n",
    "# Change this if you don't want the data to be extracted in the current directory.\n",
    "data_dir = '/workspace/data2'\n",
    "download_an4_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!! sed -n '10,20p' {data_dir}/an4/etc/an4_train.transcription\n",
    "['<s> C Z D Z W EIGHT </s> (an86-fbbh-b)',\n",
    " '<s> ENTER SIX TWO FOUR </s> (an87-fbbh-b)',\n",
    " '<s> ERASE O T H F I FIVE ZERO </s> (an88-fbbh-b)',\n",
    " '<s> RUBOUT T G J W B SEVENTY NINE FIFTY NINE </s> (an89-fbbh-b)',\n",
    " '<s> NO </s> (an90-fbbh-b)',\n",
    " '<s> H O W E L L </s> (cen1-fbbh-b)',\n",
    " '<s> B E V E R L Y </s> (cen2-fbbh-b)',\n",
    " '<s> FIFTY ONE FIFTY SIX </s> (cen3-fbbh-b)',\n",
    " '<s> P R I N C E </s> (cen4-fbbh-b)',\n",
    " '<s> G I B S O N I A </s> (cen5-fbbh-b)',\n",
    " '<s> ONE FIVE OH FOUR FOUR </s> (cen6-fbbh-b)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Jasper Model\n",
    "\n",
    "- We will be putting together a small [Jasper (Just Another SPeech Recognizer) model](https://arxiv.org/abs/1904.03288).\n",
    "- Jasper architectures consist of a repeated block structure that utilizes 1D convolutions.\n",
    "- In a Jasper_BxR model:\n",
    "    - `R` sub-blocks (consisting of a 1D convolution, batch norm, ReLU, and dropout) are grouped into a single block, which is then repeated `B` times.\n",
    "- We also have a one extra block at the beginning and a few more at the end that are invariant of `B` and `R`, and we use CTC loss.\n",
    "\n",
    "A Jasper model looks like roughly this:\n",
    "\n",
    "![Jasper with CTC](https://raw.githubusercontent.com/NVIDIA/NeMo/master/docs/sources/source/asr/jasper_vertical.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple ASR Pipeline in NeMo\n",
    "\n",
    "We'll be using the **Neural Modules (NeMo) toolkit** for this part:\n",
    "- [GitHub page](https://github.com/NVIDIA/NeMo)\n",
    "- [Documentation](https://nvidia.github.io/NeMo/)\n",
    "\n",
    "NeMo lets us easily hook together the components (modules) of our model, such as the data layer, intermediate layers, and various losses, without worrying too much about implementation details of individual parts or connections between modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeMo's \"core\" package\n",
    "import nemo\n",
    "# NeMo's ASR collection\n",
    "import nemo.collections.asr as nemo_asr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data Manifests\n",
    "\n",
    "- Manifests for our training and evaluation data, which will contain the metadata of our audio files. \n",
    "- NeMo data layers take in a standardized manifest format where each line corresponds to one sample of audio, such that the number of lines in a manifest is equal to the number of samples that are represented by that manifest. A line must contain the path to an audio file, the corresponding transcript (or path to a transcript file), and the duration of the audio sample.\n",
    "\n",
    "Here's an example of what one line in a NeMo-compatible manifest might look like:\n",
    "```\n",
    "{\"audio_filepath\": \"path/to/audio.wav\", \"duration\": 3.45, \"text\": \"this is a nemo tutorial\"}\n",
    "```\n",
    "- we can build our training and evaluation manifests using the transcription files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!! sed -n '10,20p' train_manifest.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Training and Evaluation DAGs with NeMo\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our NeuralModuleFactory, which will oversee the neural modules.\n",
    "neural_factory = nemo.core.NeuralModuleFactory( # main engine to drive the pipeline including checkpoints, callbacks, logs, and other details for training and inference\n",
    "    log_dir=data_dir+'/an4_tutorial/') # where model logs and outputs will be written\n",
    "\n",
    "logger = nemo.logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying Configuration of the Model\n",
    "\n",
    "We'll build a *Jasper_4x1 model*, with `B=4` blocks of single (`R=1`) sub-blocks and a *greedy CTC decoder*, using the configuration found in `jasper_an4.yaml`.\n",
    "\n",
    "Using a YAML config such as this is helpful for getting a quick and human-readable overview of what your architecture looks like, and allows you to swap out model and run configurations easily without needing to change your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config Information ---#\n",
    "from ruamel.yaml import YAML\n",
    "\n",
    "config_path = 'jasper_an4.yaml'\n",
    "\n",
    "yaml = YAML(typ='safe')\n",
    "with open(config_path) as f:\n",
    "    params = yaml.load(f)\n",
    "labels = params['labels'] # Vocab\n",
    "\n",
    "train_manifest = 'train_manifest.json'\n",
    "test_manifest = 'test_manifest.json'\n",
    "\n",
    "!! cat jasper_an4.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Neural Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test data layers (which load data) and data preprocessor\n",
    "data_layer_train = nemo_asr.AudioToTextDataLayer.import_from_config(\n",
    "    config_path,\n",
    "    \"AudioToTextDataLayer_train\",\n",
    "    overwrite_params={\"manifest_filepath\": train_manifest}\n",
    ") # Training datalayer\n",
    "\n",
    "data_preprocessor = nemo_asr.AudioToMelSpectrogramPreprocessor.import_from_config(\n",
    "    config_path, \"AudioToMelSpectrogramPreprocessor\"\n",
    ")\n",
    "\n",
    "# Create the Jasper_4x1 encoder as specified, and a CTC decoder\n",
    "encoder = nemo_asr.JasperEncoder.import_from_config(\n",
    "    config_path, \"JasperEncoder\"\n",
    ")\n",
    "\n",
    "decoder = nemo_asr.JasperDecoderForCTC.import_from_config(\n",
    "    config_path, \"JasperDecoderForCTC\",\n",
    "    overwrite_params={\"num_classes\": len(labels)}\n",
    ")\n",
    "\n",
    "ctc_loss = nemo_asr.CTCLossNM(num_classes=len(labels))\n",
    "greedy_decoder = nemo_asr.GreedyCTCDecoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_layer_test = nemo_asr.AudioToTextDataLayer.import_from_config(\n",
    "    config_path,\n",
    "    \"AudioToTextDataLayer_eval\",\n",
    "    overwrite_params={\"manifest_filepath\": test_manifest}\n",
    ") # Eval datalayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wire up the training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to assemble our training DAG by specifying the inputs to each neural module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Assemble Training DAG --- #\n",
    "audio_signal, audio_signal_len, transcript, transcript_len = data_layer_train()\n",
    "\n",
    "processed_signal, processed_signal_len = data_preprocessor(\n",
    "    input_signal=audio_signal,\n",
    "    length=audio_signal_len)\n",
    "\n",
    "encoded, encoded_len = encoder(\n",
    "    audio_signal=processed_signal,\n",
    "    length=processed_signal_len)\n",
    "\n",
    "log_probs = decoder(encoder_output=encoded)\n",
    "\n",
    "preds = greedy_decoder(log_probs=log_probs)  # Training predictions\n",
    "loss = ctc_loss(\n",
    "    log_probs=log_probs,\n",
    "    targets=transcript,\n",
    "    input_length=encoded_len,\n",
    "    target_length=transcript_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wire up the evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation DAG will reuse most of the parts of the training DAG with the exception of the data layer, since we are loading the evaluation data from a different file but evaluating on the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Assemble Validation DAG --- #\n",
    "(audio_signal_test, audio_len_test,\n",
    " transcript_test, transcript_len_test) = data_layer_test()\n",
    "\n",
    "processed_signal_test, processed_len_test = data_preprocessor(\n",
    "    input_signal=audio_signal_test,\n",
    "    length=audio_len_test)\n",
    "\n",
    "encoded_test, encoded_len_test = encoder(\n",
    "    audio_signal=processed_signal_test,\n",
    "    length=processed_len_test)\n",
    "\n",
    "log_probs_test = decoder(encoder_output=encoded_test)\n",
    "\n",
    "preds_test = greedy_decoder(log_probs=log_probs_test)  # Test predictions\n",
    "loss_test = ctc_loss(\n",
    "    log_probs=log_probs_test,\n",
    "    targets=transcript_test,\n",
    "    input_length=encoded_len_test,\n",
    "    target_length=transcript_len_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Model\n",
    "- We would like to be able to monitor our model while it's training, so we use **callbacks**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use these imports to pass to callbacks more complex functions to perform.\n",
    "from nemo.collections.asr.helpers import monitor_asr_train_progress, \\\n",
    "    process_evaluation_batch, process_evaluation_epoch\n",
    "from functools import partial\n",
    "\n",
    "train_callback = nemo.core.SimpleLossLoggerCallback(\n",
    "    # Notice that we pass in loss, predictions, and the transcript info.\n",
    "    # Of course we would like to see our training loss, but we need the\n",
    "    # other arguments to calculate the WER.\n",
    "    tensors=[loss, preds, transcript, transcript_len],\n",
    "    # The print_func defines what gets printed.\n",
    "    print_func=partial(\n",
    "        monitor_asr_train_progress,\n",
    "        labels=labels),\n",
    "    tb_writer=neural_factory.tb_writer\n",
    "    )\n",
    "\n",
    "# We can create as many evaluation DAGs and callbacks as we want,\n",
    "# which is useful in the case of having more than one evaluation dataset.\n",
    "# In this case, we only have one.\n",
    "eval_callback = nemo.core.EvaluatorCallback(\n",
    "    eval_tensors=[loss_test, preds_test, transcript_test, transcript_len_test],\n",
    "    user_iter_callback=partial(\n",
    "        process_evaluation_batch, labels=labels),\n",
    "    user_epochs_done_callback=process_evaluation_epoch,\n",
    "    eval_step=500,  # How often we evaluate the model on the test set\n",
    "    tb_writer=neural_factory.tb_writer\n",
    "    )\n",
    "\n",
    "checkpoint_saver_callback = nemo.core.CheckpointCallback(\n",
    "    folder=data_dir+'/an4_checkpoints',\n",
    "    step_freq=1000  # How often checkpoints are saved\n",
    "    )\n",
    "\n",
    "import os\n",
    "if not os.path.exists(data_dir+'/an4_checkpoints'):\n",
    "    os.makedirs(data_dir+'/an4_checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we create our neural factory and the callbacks for the information that we want to see, we can **start training** by simply calling the train function on the tensors we want to optimize and our callbacks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Start Training! --- #\n",
    "neural_factory.train(\n",
    "    tensors_to_optimize=[loss],\n",
    "    callbacks=[train_callback, eval_callback, checkpoint_saver_callback],\n",
    "    optimizer='novograd',\n",
    "    optimization_params={\n",
    "        \"num_epochs\": 110, \"lr\": 0.001, \"weight_decay\": 1e-4 # already run 100 epochs and here we do another 10\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inference Only --- #\n",
    "\n",
    "# We've already built the inference DAG above, so all we need is to call infer().\n",
    "evaluated_tensors = neural_factory.infer(\n",
    "    # These are the tensors we want to get from the model.\n",
    "    tensors=[loss_test, preds_test, transcript_test, transcript_len_test],\n",
    "    # checkpoint_dir specifies where the model params are loaded from.\n",
    "    checkpoint_dir=(data_dir+'/an4_checkpoints')\n",
    "    )\n",
    "\n",
    "# Process the results to get WER\n",
    "from nemo.collections.asr.helpers import word_error_rate, \\\n",
    "    post_process_predictions, post_process_transcripts\n",
    "\n",
    "greedy_hypotheses = post_process_predictions( evaluated_tensors[1], labels)\n",
    "\n",
    "references = post_process_transcripts( evaluated_tensors[2], evaluated_tensors[3], labels)\n",
    "\n",
    "wer = word_error_rate(hypotheses=greedy_hypotheses, references=references)\n",
    "\n",
    "print(\"*** WER: {:.2f} ***\".format(wer * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(greedy_hypotheses[10])\n",
    "print(references[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(greedy_hypotheses[20])\n",
    "print(references[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading/watching:\n",
    "- [Stanford Lecture on ASR](https://www.youtube.com/watch?v=3MjIkWxXigM)\n",
    "- [\"An Intuitive Explanation of Connectionist Temporal Classification\"](https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c)\n",
    "- [Explanation of CTC with Prefix Beam Search](https://medium.com/corti-ai/ctc-networks-and-language-models-prefix-beam-search-explained-c11d1ee23306)\n",
    "- [Listen Attend and Spell Paper (seq2seq ASR model)](https://arxiv.org/abs/1508.01211)\n",
    "- [Explanation of the mel spectrogram in more depth](https://towardsdatascience.com/getting-to-know-the-mel-spectrogram-31bca3e2d9d0)\n",
    "- [Jasper Paper](https://arxiv.org/abs/1904.03288)\n",
    "- [SpecAugment Paper](https://arxiv.org/abs/1904.08779)\n",
    "- [Explanation and visualization of SpecAugment](https://towardsdatascience.com/state-of-the-art-audio-data-augmentation-with-google-brains-specaugment-and-pytorch-d3d1a3ce291e)\n",
    "- [Cutout Paper](https://arxiv.org/pdf/1708.04552.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
